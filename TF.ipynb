{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark # installing pyspark."
      ],
      "metadata": {
        "id": "Ht6Ug_UPOIfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4654799-aa13-4e39-ede7-754b2b33c71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1Q-wtQmD-OE"
      },
      "outputs": [],
      "source": [
        "import sys #installing necessary packages.\n",
        "from pyspark import SparkContext, SparkConf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf()\n",
        "conf.setMaster('local')\n",
        "conf.setAppName('TF of task-1')  #setting  and initializing spark context variable.\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "0AuNogboObWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1=\"/content/Assignment3_Data.txt\"\n",
        "fileRDD = sc.textFile(file1) # parallelizing raw data into rdd\n"
      ],
      "metadata": {
        "id": "i-fg6YcLOme1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fileRDD.take(1)"
      ],
      "metadata": {
        "id": "yPwKOZEIzQfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task-1"
      ],
      "metadata": {
        "id": "knO3wxrwD4hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.data.path.append('/home/schitya/nltk_data') #should specify your nltk downloaded path here.\n",
        "stop_words = list(stopwords.words('english')) # making list of stop words to filter later\n",
        "def filtering(x):\n",
        "  y=x.split('</docid>') # as our matter is in inside text spliting till we get text.\n",
        "  z=y[1].split('</text>')\n",
        "  main=z[0].split('<text>') # whole matter will be in first index of main text.\n",
        "  #text=main[1].split(' ')\n",
        "  text_content_body =main[1].split(\" \") # splitting the main text based on space.\n",
        "  text = re.sub('\\[\\[(.*?)\\]\\]', lambda match: match.group(1), main[1]) #https://stackoverflow.com/questions/2094975/python-re-sub-question  #Now we have many [[]] and {{}} this type of words the following functions will extract the word \n",
        "  text = re.sub('\\{\\{(.*?)\\}\\}', lambda match: match.group(1), text) #https://stackoverflow.com/questions/2094975/python-re-sub-question  # inside these words.\n",
        "  text_array = text.split(\" \")\n",
        "  filtered_text = \"\"\n",
        "  for x1 in text_array:\n",
        "    if '|' in x1:\n",
        "      y1 = x1.split('|')\n",
        "      filtered_text = filtered_text + \" \".join(y1) # eleminating the strings based on '|' if they have any in the word.\n",
        "    else:\n",
        "      filtered_text = filtered_text + x1 + \" \"\n",
        "  filtered_text = filtered_text.strip() #removing empty spaces at first and at end.\n",
        "  filtered_text = filtered_text.lower() #converting all the characters to lower \n",
        "  \n",
        "  array = filtered_text.split(' ') #making the resultant string into List of words.\n",
        "\n",
        "  text_without_stopwords =\"\"\n",
        "\n",
        "  for x1 in array:\n",
        "    if x1 in stop_words:\n",
        "      continue                                     #removing stopwords means if any word is in stopwords list it is not appended as string.\n",
        "    else:\n",
        "      text_without_stopwords += x1 + \" \"   \n",
        "\n",
        "  \n",
        "  urls = re.compile(r'https?://\\S+|www\\.\\S+') #first using regexs finding websites and likns.\n",
        "  text_without_urls = urls.sub('', text_without_stopwords) # removing urls by substituing any url with no space.\n",
        "\n",
        "  text_without_punctuations = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text_without_urls) #https://www.geeksforgeeks.org/python-remove-punctuation-from-string/ #Removed punctuations.\n",
        "\n",
        "\n",
        "\n",
        "  array2 = text_without_punctuations.split(' ')\n",
        "  #regex = re.compile('[^a-z]')                              #https://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
        "  useful_text = \"\"\n",
        " \n",
        "  for x1 in array2:\n",
        "    useful_text = useful_text + \" \" + str(re.sub(\"[^a-zA-Z]+\", \"\", x1)) # after all necessary text processing I am making useful string.\n",
        "  \n",
        "  #print(y[0])\n",
        "  #print(useful_text)\n",
        "  #print(y[1])\n",
        "\n",
        "  id_array = y[0].split('<docid>')\n",
        "  id = id_array[1]\n",
        "  return (id, useful_text) # I am returning as tuple where first element is document id and second element is text for further tasks.\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "ziQhMPQIr0pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1454eec3-e9f0-4bf6-e362-e9ec4e94b418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task1RDD = fileRDD.map(lambda x : filtering(x)) #whole articles are filtered as rdd of tuples in which first element is document id and second element is processed text.\n",
        "#task1RDD = task1RDD.flatMap(lambda x : (x[0],x[1]))"
      ],
      "metadata": {
        "id": "OYQRwui84kow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#task1RDD.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-COTKAcNZ0T",
        "outputId": "1294297d-b6f1-45bc-9483-eaefa75d722d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('10000',\n",
              "  ' siege jerusalem    unsourced date august    siege jerusalem    took place september  october     part war called crusades thirdcrusade   balian ibelin defended jerusalem armies saladin  surrendered  muslims enslaved thousands christians let many leave bought freedom   fictionalized version siege  attack  seen movie   kingdom heaven    directed ridley scott   commons category siegeof jerusalem    military stub  category battles middle ages jerusalem    siege category crusades category  ')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-2"
      ],
      "metadata": {
        "id": "2RARI8MMD_hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def term_frequency_construction(x):\n",
        "  content = x[1] # here we get x as tuples in which first element is document id and second element is text content.\n",
        "  word_frequency = {} # dictionary for calculating the frequency of words.\n",
        "  content_array= content.split(\" \")\n",
        "  for y in content_array: #splitting based on the empty space for frequency calculation.\n",
        "    if(len(y)>0):\n",
        "      #word_frequency[y] +=1\n",
        "      if y not in word_frequency:\n",
        "        word_frequency[y]=1           #first frequency of words are calculated by using dictionary in which if words are not there means assigning it's respective value to be 1 and incrementing the value if key is found\n",
        "      else:\n",
        "        word_frequency[y] +=1\n",
        "        \n",
        "  for y in word_frequency:\n",
        "    count = word_frequency[y] \n",
        "    freq = 1+ math.log(count,10) # for every element of dictionary applying 1+ log value.\n",
        "    word_frequency[y]= freq\n",
        "  \"\"\"word_count=[]\n",
        "  for y in word_frequency:\n",
        "    element=(y,word_frequency[y])\n",
        "    word_count.append(element)\"\"\"\n",
        "\n",
        "  return (x[0], word_frequency) #returning document id in first element of tuple and dictionary of 1+log(freq) values and words as keys in second element \n"
      ],
      "metadata": {
        "id": "MDF7gXFdDLFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2RDD = task1RDD.map(lambda x : term_frequency_construction(x)) #constructing term frequecy matrix and we get rdd of tuples of first element as doc-id and second element as dicitonary of words with log of frequency values.\n",
        "step2RDD = step2RDD.flatMap(lambda x : [(word, x[0], x[1][word]) for word in x[1]]) #Now we need every word in dictionary of every element of tuple as key so for that I used for loop in lambda function and for every word I made tuple of word in dictionary of every element of rdd, document-id as second element and value as third element.\n",
        "step2RDD = step2RDD.map(lambda x: (x[0],(x[1],x[2])))#After flatting I made every a tuple like (word, (doc-id,freq))\n",
        "step2RDD = step2RDD.map(lambda x: (x[0], [x[1]]))  #After above transformation I will have rdd of many elements like of the format (word,(doc-id,freq)) that means I can have same word as key with different tuple as value so I am making list for values to add for same key.\n",
        "step2RDD = step2RDD.reduceByKey(lambda a, b: a + b) #For same key I will add (doc-id,freq) tuples to the list that I have created above.\n",
        "\n",
        "#task2RDD = task2RDD.map(lambda x : (x[0], [(x[0], v) for v in x[1:][0]]))"
      ],
      "metadata": {
        "id": "c9WG4QOMGgbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#task2RDD.take(1)"
      ],
      "metadata": {
        "id": "VkMyDHE3SZml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversion(x):\n",
        "  first_part = str(x[0]) + \"@\" #we get every line as tuple of the form (word,[(doc-1,freq),(doc-2,freq).......])\n",
        "  remaining_part = \"\"\n",
        "  for y in x[1]: \n",
        "    if len(y)==2:\n",
        "      t=\"\"\n",
        "      t = \"doc\" + \"-\" + y[0] + \"#\" + str(y[1]) + \"+\" # using for loop to convert in required format.\n",
        "      remaining_part = remaining_part + t\n",
        "    main = first_part + remaining_part\n",
        "\n",
        "  main = main[:-1] #removing last \"+\" that will be added in the for loop at end.\n",
        "  return main\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_XuULon3JWiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2RDD = step2RDD.map(lambda x : conversion(x)) # every rdd element is converted to required string format."
      ],
      "metadata": {
        "id": "0B0HZ-2pOz-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step2RDD.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MVc31V5GvZo",
        "outputId": "2dd96cb6-f6d9-4c4d-e904-8664dec57c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['siege@doc-10000#1.6020599913279623+doc-10025#1.0+doc-10075#1.0+doc-10087#1.0+doc-10285#1.0+doc-10310#1.3010299956639813+doc-10332#1.3010299956639813+doc-10357#1.0+doc-11346#1.0+doc-11347#1.0+doc-11493#1.0+doc-11602#1.0+doc-12051#1.0+doc-12197#1.0+doc-12287#1.0+doc-12595#1.0+doc-12685#1.0+doc-12765#1.3010299956639813+doc-12766#1.0+doc-12942#1.0+doc-13245#1.0+doc-13630#1.0+doc-13667#1.3010299956639813+doc-14620#1.0+doc-14922#1.0+doc-15070#1.0+doc-15207#1.0+doc-15221#1.0+doc-15318#1.3010299956639813+doc-16021#1.0+doc-16084#1.0+doc-16097#1.4771212547196624+doc-16147#1.0+doc-16413#1.0+doc-16862#1.0+doc-16910#1.4771212547196624+doc-17017#1.0+doc-17084#1.0+doc-17408#1.0+doc-17415#1.0+doc-17465#1.0+doc-17577#1.0+doc-17583#1.4771212547196624+doc-18912#1.0+doc-19476#1.0+doc-19585#1.0+doc-19711#1.0+doc-19865#1.3010299956639813']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_path = \"/content/TF_index\"\n",
        "step2RDD.saveAsTextFile(folder_path) # rdd is writen to specified folder for using for TF_Query."
      ],
      "metadata": {
        "id": "ifbOhSb8YeI2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}