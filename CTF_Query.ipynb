{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A4vP7kMjDME",
        "outputId": "f07a751c-0c14-4b60-d2b6-c9eed466eaae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=885e28bd6684e0906e3e21a21cef273b6da5612de0199998fc645fe64b18d842\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark # pyspark is installed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf #importing packages."
      ],
      "metadata": {
        "id": "ss-FGmPMj6im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf()\n",
        "conf.setMaster('local')\n",
        "conf.setAppName('CTF_query of task-2')  #spark session is intiated and activated.\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "cO4gzOgzkAQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file2=\"/content/CTF_index_matrix\"\n",
        "fileRDD = sc.textFile(file2)   #parallelizing the file into rdd."
      ],
      "metadata": {
        "id": "1H8gzrdRkNQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversion_to_rdd(x): #this process is used to convert back to rdd.\n",
        "  y = x.split('@') #as we have every line wrod@doc-id1#freq1..... splitting on @ to get word in first element of list and remaining in second part.\n",
        "  z = y[1].split('+') # Again splitting on + as to represent 2 different documents.\n",
        "  document_dictionary ={} #initialinzing dictionary for storing doc-id as key and freq as value.\n",
        "  for t in z :\n",
        "    n = t.split('#')\n",
        "    document_dictionary[n[0]] = float(n[1]) #making freq to floats for further processing \n",
        "\n",
        "  return (y[0],document_dictionary) #returning a tuple in which first element is word and second element is dictionary of docids as keys and freq as values."
      ],
      "metadata": {
        "id": "vMz2sAdTkYjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CTFRDD = fileRDD.map(lambda x : conversion_to_rdd(x)) # rdd has every element of tuples in the mentioned function."
      ],
      "metadata": {
        "id": "c6oec-uzkfHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CTFRDD.take(1)"
      ],
      "metadata": {
        "id": "2P2CivzjklDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Query = input('Enter your query for CTF :- ') #query is asked from user."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jecQM2QIktAC",
        "outputId": "eb046a6f-85f1-4f0e-96b1-9c34258d4ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query for CTF :- JERUSALEM loves MUSLIM regarding nazi in www.goole.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tkinter.constants import X\n",
        "import re\n",
        "import string #necessary packages are imported.\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = list(stopwords.words('english')) #stopwords list is created for removal .\n",
        "def preprocessing(x):\n",
        "   x = x.lower()  #query words are lowered \n",
        "   array1 = x.split(\" \") #splitted based on empty space.\n",
        "   y = \"\"\n",
        "   for x1 in array1: \n",
        "     if x1 in stop_words: #stopwords are removed if they are in the list of stopwords.\n",
        "       continue\n",
        "     else:\n",
        "       y = y+ \" \"+ x1 + \" \"\n",
        "\n",
        "   urls = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "   text_without_urls = urls.sub('', y) #query urls are removed\n",
        "\n",
        "   text_without_punctuations = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text_without_urls) #https://www.geeksforgeeks.org/python-remove-punctuation-from-string/ #query punctuations are removed.\n",
        "\n",
        "   useful_words = set()\n",
        "   array2 = text_without_punctuations.split(\" \")\n",
        "   for x1 in array2:\n",
        "     if len(x1)>0:  #I used set to add filtered words.\n",
        "       useful_words.add(x1)\n",
        "\n",
        "   return useful_words #that set is returned.\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkKy5e_llbQe",
        "outputId": "a787c1ce-e700-4426-c701-031c3bf1ca5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words  = preprocessing(Query) #query is preprocessed."
      ],
      "metadata": {
        "id": "zCEDpHOOl3r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(words)"
      ],
      "metadata": {
        "id": "U3ZhVaEDmRyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CTFRDD = CTFRDD.filter(lambda x : x[0] in words) #filtering our rdd only when words are in the set."
      ],
      "metadata": {
        "id": "O357WAhwmW6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CTFRDD.take(1)"
      ],
      "metadata": {
        "id": "dWYyjabhmpYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def construction_of_IDF_and_TFIDF(x):\n",
        "  no_of_documents_with_one_word = len(x[1]) #length of dictionary to calculate IDF which gives you no of documents the word is present.\n",
        "  top =10000/(1+ no_of_documents_with_one_word) # here idf is calculated for every word and 10000 is standard total documents which is given in the question.\n",
        "  IDF = math.log(top,10) \n",
        "\n",
        "  output = []\n",
        "  for y in x[1]:\n",
        "    TF_IDF_of_particular_document = x[1][y] * IDF\n",
        "    per_document = (y,IDF,TF_IDF_of_particular_document) #for every value TFIDF is calculated  and overridden in dictionary.\n",
        "    output.append(per_document)\n",
        "  return tuple(output) #returning as tuple of list of tuples in the form (doc-id,IDF,TFIDF OF every document)\n",
        "\n"
      ],
      "metadata": {
        "id": "zPgCI2Lj9XMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CTFRDD = CTFRDD.map(lambda x : construction_of_IDF_and_TFIDF(x)) #rdd will be of the tuple of tuple form (((doc-id,IDF,TFIDF),(doc-id2,IDF2)...))"
      ],
      "metadata": {
        "id": "Gwz8MvebCSHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CTFRDD.take(4)"
      ],
      "metadata": {
        "id": "Y5c_YpkVbTJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversion_to_dictionary(x):\n",
        "  output ={} #we get every line as (((doc-id,IDF,TFIDF),(doc-id2,IDF2)...))\n",
        "  for y in x :\n",
        "    values=(y[1],y[2])\n",
        "    output[y[0]] = values\n",
        "  return output #we wrill return dictionary in which elements will be of the form {doc-id:(IDF,TFIDF)}\n"
      ],
      "metadata": {
        "id": "l8l7sS4WHaLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CTFRDD.take(1)\n",
        "CTFRDD = CTFRDD.map(lambda x : conversion_to_dictionary(x)) #rdd is mapped based on the return values of convert_to_dictionary function.\n"
      ],
      "metadata": {
        "id": "2ipuAa3DCfv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CTFRDD.take(1)\n",
        "CTFRDD = CTFRDD.flatMap(lambda x: [(doc_id, x[doc_id]) for doc_id  in x]) #As we have every element of the form tuple of the form (word,{doc-id1:(IDF,TFIDF)}...) But our motive is to get documents so iterating in lambda function and taking that key,value pair as tuple and flattening the rdd.\n",
        "CTFRDD = CTFRDD.groupByKey() ## for same doc-id as key we group all tuples of the form (IDF,TFIDF) \n",
        "CTFRDD = CTFRDD.mapValues(list) # we make the iterator of every key to be list."
      ],
      "metadata": {
        "id": "xK7ZVsw_ETt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CTFRDD.take(1)"
      ],
      "metadata": {
        "id": "l59nREdtI2-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DOT product part"
      ],
      "metadata": {
        "id": "74Vpus7II9me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Dot_product(x):\n",
        "  dot_product = 0\n",
        "  for y in x[1]:\n",
        "    ct   = y[0]\n",
        "    ctfd = y[1]\n",
        "    dot_product += ct*ctfd #performing dot product and adding all the vectors.\n",
        "  return (x[0],dot_product)\n"
      ],
      "metadata": {
        "id": "8vEO5j5TKGWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CTFRDD = CTFRDD.map(lambda x : Dot_product(x)) #we get rdd of elements of type (doc-id, dot producted value) after calling the function."
      ],
      "metadata": {
        "id": "gjQ8X9-pK7M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OutputRDD = CTFRDD.sortBy(lambda x: x[1], ascending=False) #from pyspark documentation ##from pyspark documentation #using lambda function based on values sorted in descending order.\n",
        "OutputRDD = OutputRDD.zipWithIndex() # To get first 10 elements of RDD without getting out of rdd used zipindex which gives indexing for every elements.\n",
        "OutputRDD = OutputRDD.filter(lambda x : x[1]<=9) # taking first 10 document ids based on ids.\n",
        "OutputRDD = OutputRDD.map(lambda x : (x[0][0],x[0][1])) #taking as tuples.\n"
      ],
      "metadata": {
        "id": "WXHG4e_uLIfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OutputRDD.take(2)"
      ],
      "metadata": {
        "id": "su8M-G5fLodq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_path = \"/content/qID\"\n",
        "OutputRDD.saveAsTextFile(folder_path) #writing back to specifed folder."
      ],
      "metadata": {
        "id": "dWIbVEzaLouH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}