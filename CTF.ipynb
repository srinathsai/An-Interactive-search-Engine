{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark # installing pyspark."
      ],
      "metadata": {
        "id": "Ht6Ug_UPOIfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342e32e6-0012-45a6-d0f0-222c042c0058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=9b9cc98b2147520e4e77869c63f07394727363601cc6de283f89d4225bb02ada\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1Q-wtQmD-OE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf #installing necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf()\n",
        "conf.setMaster('local')\n",
        "conf.setAppName('CTF of task-1') #setting  and initializing spark context variable.\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "0AuNogboObWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1=\"/content/Assignment3_Data.txt\"\n",
        "fileRDD = sc.textFile(file1) # parallelizing raw data into rdd\n"
      ],
      "metadata": {
        "id": "i-fg6YcLOme1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fileRDD.count()"
      ],
      "metadata": {
        "id": "yPwKOZEIzQfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task-1"
      ],
      "metadata": {
        "id": "knO3wxrwD4hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.data.path.append('/home/schitya/nltk_data') #should specify your nltk downloaded path here.\n",
        "stop_words = list(stopwords.words('english')) # making list of stop words to filter later\n",
        "def filtering(x):\n",
        "  y=x.split('</docid>') \n",
        "  z=y[1].split('</text>')\n",
        "  main=z[0].split('<text>') # whole matter will be in first index of main text.\n",
        "  #text=main[1].split(' ')\n",
        "  text_content_body =main[1].split(\" \") # splitting the main text based on space.\n",
        "  \n",
        "  text = re.sub('\\[\\[(.*?)\\]\\]', lambda match: match.group(1), main[1]) #https://stackoverflow.com/questions/2094975/python-re-sub-question #Now we have many [[]] and {{}} this type of words the following functions will extract the word \n",
        "  text = re.sub('\\{\\{(.*?)\\}\\}', lambda match: match.group(1), text) #https://stackoverflow.com/questions/2094975/python-re-sub-question  # inside these words.\n",
        "  text_array = text.split(\" \")\n",
        "  filtered_text = \"\"\n",
        "  for x1 in text_array:\n",
        "    if '|' in x1:\n",
        "      y1 = x1.split('|')\n",
        "      filtered_text = filtered_text + \" \".join(y1)  # eleminating the strings based on '|' if they have any in the word.\n",
        "    else:\n",
        "      filtered_text = filtered_text + x1 + \" \"\n",
        "  filtered_text = filtered_text.strip() #removing empty spaces at first and at end.\n",
        "  filtered_text = filtered_text.lower() #converting all the characters to lower \n",
        "  \n",
        "  array = filtered_text.split(' ')  #making the resultant string into List of words.\n",
        "\n",
        "  text_without_stopwords =\"\"\n",
        "\n",
        "  for x1 in array:\n",
        "    if x1 in stop_words:\n",
        "      continue                           #removing stopwords means if any word is in stopwords list it is not appended as string.\n",
        "    else:\n",
        "      text_without_stopwords += x1 + \" \"\n",
        "\n",
        "  \n",
        "  urls = re.compile(r'https?://\\S+|www\\.\\S+') #first using regexs finding websites and likns.\n",
        "  text_without_urls = urls.sub('', text_without_stopwords) # removing urls by substituing any url with no space.\n",
        "\n",
        "  text_without_punctuations = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text_without_urls) #https://www.geeksforgeeks.org/python-remove-punctuation-from-string/ #Removed punctuations.\n",
        "\n",
        "\n",
        "\n",
        "  array2 = text_without_punctuations.split(' ') \n",
        "  #regex = re.compile('[^a-z]')                              #https://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
        "  useful_text = \"\"\n",
        " \n",
        "  for x1 in array2:\n",
        "    useful_text = useful_text + \" \" + str(re.sub(\"[^a-zA-Z]+\", \"\", x1)) # after all necessary text processing I am making useful string.\n",
        "  \n",
        "  #print(y[0])\n",
        "  #print(useful_text)\n",
        "  #print(y[1])\n",
        "\n",
        "  id_array = y[0].split('<docid>') \n",
        "  id = id_array[1]\n",
        "  return (id, useful_text) # I am returning as tuple where first element is document id and second element is text for further tasks.\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "ziQhMPQIr0pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d23bfa-9b79-4d5e-930e-649398c84495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task1RDD = fileRDD.map(lambda x : filtering(x)) #whole articles are filtered as rdd of tuples in which first element is document id and second element is processed text.\n",
        "#task1RDD = task1RDD.flatMap(lambda x : (x[0],x[1]))"
      ],
      "metadata": {
        "id": "OYQRwui84kow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-2"
      ],
      "metadata": {
        "id": "2RARI8MMD_hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def term_frequency_construction(x):\n",
        "  content = x[1] # here we get x as tuples in which first element is document id and second element is text content.\n",
        "  word_frequency = {} # dictionary for calculating the frequency of words.\n",
        "  content_array= content.split(\" \") #splitting based on the empty space for frequency calculation.\n",
        "  for y in content_array:\n",
        "    if(len(y)>0):\n",
        "      if y not in word_frequency:  \n",
        "        word_frequency[y]=1\n",
        "      else:                            #first frequency of words are calculated by using dictionary in which if words are not there means assigning it's respective value to be 1 and incrementing the value if key is found\n",
        "        word_frequency[y] +=1\n",
        "\n",
        "  sum_of_square_of_euclidean_distance =0 \n",
        "  for y in word_frequency:\n",
        "    sum_of_square_of_euclidean_distance += word_frequency[y]*word_frequency[y]  # for every value in dictionary calculating sum of square of euclidean distances.\n",
        "  euclidean_distance = math.sqrt(sum_of_square_of_euclidean_distance) # sqrt of all sum of squares is done for exact euclidean distance.\n",
        "\n",
        "  for y in word_frequency:\n",
        "    count = word_frequency[y] \n",
        "    freq = 1+ math.log(count,10)\n",
        "    normalized_freq = freq/euclidean_distance #every value in dictionary is normalized by calculated euclidean distances. \n",
        "    word_frequency[y]= normalized_freq\n",
        " \n",
        "\n",
        "  return (x[0], word_frequency) #returning document id in first element of tuple and dictionary of (1+log(freq))/euclidean_distance values and words as keys in second element \n",
        "\n"
      ],
      "metadata": {
        "id": "MDF7gXFdDLFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2RDD = task1RDD.map(lambda x : term_frequency_construction(x))#constructing term frequecy matrix and we get rdd of tuples of first element as doc-id and second element as dicitonary of words with normalization of log of frequency values.\n",
        "step2RDD = step2RDD.flatMap(lambda x : [(word, x[0], x[1][word]) for word in x[1]]) #Now we need every word in dictionary of every element of tuple as key so for that I used for loop in lambda function and for every word I made tuple of word in dictionary of every element of rdd, document-id as second element and value as third element.\n",
        "step2RDD = step2RDD.map(lambda x: (x[0],(x[1],x[2]))) #After flatting I made every a tuple like (word, (doc-id,freq))\n",
        "step2RDD = step2RDD.map(lambda x: (x[0], [x[1]])) #After above transformation I will have rdd of many elements like of the format (word,(doc-id,freq)) that means I can have same word as key with different tuple as value so I am making list for values to add for same key.\n",
        "step2RDD = step2RDD.reduceByKey(lambda a, b: a + b) ##For same key I will add (doc-id,freq) tuples to the list that I have created above.\n",
        "\n",
        "#task2RDD = task2RDD.map(lambda x : (x[0], [(x[0], v) for v in x[1:][0]]))"
      ],
      "metadata": {
        "id": "c9WG4QOMGgbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#task2RDD.take(1)"
      ],
      "metadata": {
        "id": "VkMyDHE3SZml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversion(x): #we get every line as tuple of the form (word,[(doc-1,freq),(doc-2,freq).......])\n",
        "  first_part = str(x[0]) + \"@\"\n",
        "  remaining_part = \"\" \n",
        "  for y in x[1]:\n",
        "    if len(y)==2:\n",
        "      t=\"\"\n",
        "      t = \"doc\" + \"-\" + y[0] + \"#\" + str(y[1]) + \"+\"  # using for loop to convert in required format.\n",
        "      remaining_part = remaining_part + t\n",
        "    main = first_part + remaining_part            \n",
        "\n",
        "  main = main[:-1] #removing last \"+\" that will be added in the for loop at end.\n",
        "  return main\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_XuULon3JWiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2RDD = step2RDD.map(lambda x : conversion(x)) # every rdd element is converted to required string format."
      ],
      "metadata": {
        "id": "0B0HZ-2pOz-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step2RDD.take(1)"
      ],
      "metadata": {
        "id": "1MVc31V5GvZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_path = \"/content/CTF_index\"\n",
        "step2RDD.saveAsTextFile(folder_path) # rdd is writen to specified folder for using for TF_Query."
      ],
      "metadata": {
        "id": "ifbOhSb8YeI2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}