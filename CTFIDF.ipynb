{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6zT2b9davMq",
        "outputId": "ee1ac322-fa2e-4e71-8af2-8e45c0418fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=769f7b6642c7489ee463306cca248cde31fafe93c5084ef9e009fd8edacffb62\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark # installing pyspark."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf #installing necessary packages."
      ],
      "metadata": {
        "id": "qITDhy5og965"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf()\n",
        "conf.setMaster('local')\n",
        "conf.setAppName('CTFIDF of task-1') #setting  and initializing spark context variable.\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "ZTHdtIGkhNCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1=\"/content/Assignment3_Data.txt\"\n",
        "fileRDD = sc.textFile(file1) # parallelizing raw data into rdd"
      ],
      "metadata": {
        "id": "B4vaE8JGhfQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "#nltk.data.path.append('/home/schitya/nltk_data') #should specify your nltk downloaded path here.\n",
        "stop_words = list(stopwords.words('english')) # making list of stop words to filter later\n",
        "def filtering(x):\n",
        "  y=x.split('</docid>') \n",
        "  z=y[1].split('</text>')\n",
        "  main=z[0].split('<text>') # whole matter will be in first index of main text.\n",
        "  #text=main[1].split(' ')\n",
        "  text_content_body =main[1].split(\" \") # splitting the main text based on space.\n",
        "  \n",
        "  text = re.sub('\\[\\[(.*?)\\]\\]', lambda match: match.group(1), main[1]) #https://stackoverflow.com/questions/2094975/python-re-sub-question #Now we have many [[]] and {{}} this type of words the following functions will extract the word \n",
        "  text = re.sub('\\{\\{(.*?)\\}\\}', lambda match: match.group(1), text) #https://stackoverflow.com/questions/2094975/python-re-sub-question  # inside these words.\n",
        "  text_array = text.split(\" \")\n",
        "  filtered_text = \"\"\n",
        "  for x1 in text_array:\n",
        "    if '|' in x1:\n",
        "      y1 = x1.split('|')\n",
        "      filtered_text = filtered_text + \" \".join(y1)  # eleminating the strings based on '|' if they have any in the word.\n",
        "    else:\n",
        "      filtered_text = filtered_text + x1 + \" \"\n",
        "  filtered_text = filtered_text.strip() #removing empty spaces at first and at end.\n",
        "  filtered_text = filtered_text.lower() #converting all the characters to lower \n",
        "  \n",
        "  array = filtered_text.split(' ')  #making the resultant string into List of words.\n",
        "\n",
        "  text_without_stopwords =\"\"\n",
        "\n",
        "  for x1 in array:\n",
        "    if x1 in stop_words:\n",
        "      continue                           #removing stopwords means if any word is in stopwords list it is not appended as string.\n",
        "    else:\n",
        "      text_without_stopwords += x1 + \" \"\n",
        "\n",
        "  \n",
        "  urls = re.compile(r'https?://\\S+|www\\.\\S+') #first using regexs finding websites and likns.\n",
        "  text_without_urls = urls.sub('', text_without_stopwords) # removing urls by substituing any url with no space.\n",
        "\n",
        "  text_without_punctuations = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text_without_urls) #https://www.geeksforgeeks.org/python-remove-punctuation-from-string/ #Removed punctuations.\n",
        "\n",
        "\n",
        "\n",
        "  array2 = text_without_punctuations.split(' ') \n",
        "  #regex = re.compile('[^a-z]')                              #https://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
        "  useful_text = \"\"\n",
        " \n",
        "  for x1 in array2:\n",
        "    useful_text = useful_text + \" \" + str(re.sub(\"[^a-zA-Z]+\", \"\", x1)) # after all necessary text processing I am making useful string.\n",
        "  \n",
        "  #print(y[0])\n",
        "  #print(useful_text)\n",
        "  #print(y[1])\n",
        "\n",
        "  id_array = y[0].split('<docid>') \n",
        "  id = id_array[1]\n",
        "  return (id, useful_text) # I am returning as tuple where first element is document id and second element is text for further tasks.\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GriZeUEMh2Mh",
        "outputId": "87a011da-bfc2-4b00-c883-d8ab3c4635ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bonustaskRDD = fileRDD.map(lambda x : filtering(x)) #whole articles are filtered as rdd of tuples in which first element is document id and second element is processed text."
      ],
      "metadata": {
        "id": "tqncjwMriLJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def term_frequency_construction(x):\n",
        "  content = x[1] # here we get x as tuples in which first element is document id and second element is text content.\n",
        "  word_frequency = {} # dictionary for calculating the frequency of words.\n",
        "  content_array= content.split(\" \")\n",
        "  for y in content_array: #splitting based on the empty space for frequency calculation.\n",
        "    if(len(y)>0):\n",
        "      #word_frequency[y] +=1\n",
        "      if y not in word_frequency:\n",
        "        word_frequency[y]=1           #first frequency of words are calculated by using dictionary in which if words are not there means assigning it's respective value to be 1 and incrementing the value if key is found\n",
        "      else:\n",
        "        word_frequency[y] +=1\n",
        "        \n",
        "  for y in word_frequency:\n",
        "    count = word_frequency[y] \n",
        "    freq = 1+ math.log(count,10) # for every element of dictionary applying 1+ log value.\n",
        "    word_frequency[y]= freq\n",
        "  \"\"\"word_count=[]\n",
        "  for y in word_frequency:\n",
        "    element=(y,word_frequency[y])\n",
        "    word_count.append(element)\"\"\"\n",
        "\n",
        "  return (x[0], word_frequency) #returning document id in first element of tuple and dictionary of 1+log(freq) values and words as keys in second element \n"
      ],
      "metadata": {
        "id": "Juw8n38ciSUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2RDD = bonustaskRDD.map(lambda x : term_frequency_construction(x))"
      ],
      "metadata": {
        "id": "zbSSlEubiixb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step2RDD.take(1)"
      ],
      "metadata": {
        "id": "Vl2QIvbVivnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculation_IDF(x):\n",
        "  word_dictionary = x[1]\n",
        "  total_documents_consisting_the_document = len(word_dictionary) #length of dictionary will give you how many documents the word is present.\n",
        "  top =10000/(1+ total_documents_consisting_the_document) #10000 is standard value as total number of articles .\n",
        "  IDF = math.log(top,10)\n",
        "  return (x[0],word_dictionary,IDF)                #here IDF is Calucalted and returned as tuple in the form (doc-id,dictionary,IDF)"
      ],
      "metadata": {
        "id": "xZ3JhB_8jtHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2RDD = step2RDD.map(lambda x : calculation_IDF(x))\n",
        "#RDD will be transformed into tuples of the above function returned type."
      ],
      "metadata": {
        "id": "osTfv0uwk6AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step2RDD.take(1)"
      ],
      "metadata": {
        "id": "PGzVVLYylHqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculation_TF_IDF(x):\n",
        "  IDF = x[2]\n",
        "  word_dictionary = x[1]\n",
        "  for y in word_dictionary:\n",
        "    word_dictionary[y] = word_dictionary[y] * IDF      #Here TDFIDF is calculated by above IDF calculated value and it is iterated over dictionary to over ride the values present in it.\n",
        "  return (x[0],word_dictionary)                       "
      ],
      "metadata": {
        "id": "g1wF3kjylN2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2RDD = step2RDD.map(lambda x : calculation_TF_IDF(x))"
      ],
      "metadata": {
        "id": "oEEcde-HnAIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step2RDD.take(1)\n",
        "def cosine_normalization(x):\n",
        "  word_dictionary = x[1]\n",
        "  sum_of_square_of_euclidean_distance =0\n",
        "  for y in word_dictionary:\n",
        "    sum_of_square_of_euclidean_distance += word_dictionary[y]*word_dictionary[y]      #sum of squared distances is caluculated by iterating dictionary.\n",
        "  euclidean_distance = math.sqrt(sum_of_square_of_euclidean_distance) #eucliadean distance is calculated.\n",
        "\n",
        "  for y in word_dictionary:\n",
        "    word_dictionary[y] = word_dictionary[y]/euclidean_distance #dictionary values are over ridden by dividing the values with euclidean distances.\n",
        "  \n",
        "  return (x[0], word_dictionary)\n",
        "#returned tuple(doc-id, word frequency dictionary of idftfdf cosine normalized values)."
      ],
      "metadata": {
        "id": "ELLc3_TQnRhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2RDD = step2RDD.map(lambda x : cosine_normalization(x)) #rdd is transformed from the above function return type."
      ],
      "metadata": {
        "id": "1o_V2PmMqAb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step2RDD.take(1)"
      ],
      "metadata": {
        "id": "Yo1ilW4TqAmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2RDD = step2RDD.flatMap(lambda x : [(word, x[0], x[1][word]) for word in x[1]]) #Now we need every word in dictionary of every element of tuple as key so for that I used for loop in lambda function and for every word I made tuple of word in dictionary of every element of rdd, document-id as second element and value as third element.\n",
        "step2RDD = step2RDD.map(lambda x: (x[0],(x[1],x[2]))) #After flatting I made every a tuple like (word, (doc-id,freq))\n",
        "step2RDD = step2RDD.map(lambda x: (x[0], [x[1]])) #After above transformation I will have rdd of many elements like of the format (word,(doc-id,freq)) that means I can have same word as key with different tuple as value so I am making list for values to add for same key.\n",
        "step2RDD = step2RDD.reduceByKey(lambda a, b: a + b) ##For same key I will add (doc-id,freq) tuples to the list that I have created above.\n"
      ],
      "metadata": {
        "id": "d-lzavD7rOMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversion(x): #we get every line as tuple of the form (word,[(doc-1,freq),(doc-2,freq).......])\n",
        "  first_part = str(x[0]) + \"@\"\n",
        "  remaining_part = \"\" \n",
        "  for y in x[1]:\n",
        "    if len(y)==2:\n",
        "      t=\"\"\n",
        "      t = \"doc\" + \"-\" + y[0] + \"#\" + str(y[1]) + \"+\"  # using for loop to convert in required format.\n",
        "      remaining_part = remaining_part + t\n",
        "    main = first_part + remaining_part            \n",
        "\n",
        "  main = main[:-1] #removing last \"+\" that will be added in the for loop at end.\n",
        "  return main\n"
      ],
      "metadata": {
        "id": "64F1EuxtrVEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step2RDD = step2RDD.map(lambda x : conversion(x)) # every rdd element is converted to required string format."
      ],
      "metadata": {
        "id": "Xnk5vEAVrZX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step2RDD.take(1)"
      ],
      "metadata": {
        "id": "Fdo6XaaDrc_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_path = \"/content/CTFIDF_index\"\n",
        "step2RDD.saveAsTextFile(folder_path) # rdd is writen to specified folder for using for TF_Query."
      ],
      "metadata": {
        "id": "LenTnXjZrt40"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}