{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRc3mCc9RmuA",
        "outputId": "80a0bbd5-975c-4712-8fda-e58765cf8768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark # pyspark is installed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf #importing packages."
      ],
      "metadata": {
        "id": "A-r-YFGES9YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf()\n",
        "conf.setMaster('local')\n",
        "conf.setAppName('TF_query of task-1') #spark session is intiated and activated.\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "WuOqVSudTB9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file2=\"/content/TF_index_matrix\"\n",
        "fileRDD = sc.textFile(file2) #parallelizing the file into rdd."
      ],
      "metadata": {
        "id": "s31HhEjETyTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fileRDD.take(1)"
      ],
      "metadata": {
        "id": "iMK-41IFUo00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversion_to_rdd(x): #this process is used to convert back to rdd.\n",
        "  y = x.split('@') #as we have every line wrod@doc-id1#freq1..... splitting on @ to get word in first element of list and remaining in second part.\n",
        "  z = y[1].split('+') # Again splitting on + as to represent 2 different documents.\n",
        "  document_dictionary ={} #initialinzing dictionary for storing doc-id as key and freq as value.\n",
        "  for t in z :\n",
        "    n = t.split('#') \n",
        "    document_dictionary[n[0]] = float(n[1]) #making freq to floats for further processing \n",
        "\n",
        "  return (y[0],document_dictionary) #returning a tuple in which first element is word and second element is dictionary of docids as keys and freq as values."
      ],
      "metadata": {
        "id": "V03POtXcUwkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = fileRDD.map(lambda x : conversion_to_rdd(x)) # rdd has every element of tuples in the mentioned function."
      ],
      "metadata": {
        "id": "XofWGPnWZafp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RDD.take(1)"
      ],
      "metadata": {
        "id": "BCWt1kxQZiIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Query = input('Enter your query :- ') #query is asked from user."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbIXHDV-D0HY",
        "outputId": "53792387-5bc1-4476-bcbb-dd846fcd68ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query :- JerusAlem LOVES muslim and hates NA1ZI  in www.google.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tkinter.constants import X\n",
        "import re\n",
        "import string #necessary packages are imported.\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = list(stopwords.words('english')) #stopwords list is created for removal .\n",
        "def preprocessing(x):\n",
        "   x = x.lower()  #query words are lowered \n",
        "   array1 = x.split(\" \") #splitted based on empty space.\n",
        "   y = \"\"\n",
        "   for x1 in array1: \n",
        "     if x1 in stop_words: #stopwords are removed if they are in the list of stopwords.\n",
        "       continue\n",
        "     else:\n",
        "       y = y+ \" \"+ x1 + \" \"\n",
        "\n",
        "   urls = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "   text_without_urls = urls.sub('', y) #query urls are removed\n",
        "\n",
        "   text_without_punctuations = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text_without_urls) #https://www.geeksforgeeks.org/python-remove-punctuation-from-string/ #query punctuations are removed.\n",
        "\n",
        "   useful_words = set()\n",
        "   array2 = text_without_punctuations.split(\" \")\n",
        "   for x1 in array2:\n",
        "     if len(x1)>0:  #I used set to add filtered words.\n",
        "       useful_words.add(x1)\n",
        "\n",
        "   return useful_words #that set is returned.\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re4t0jHsEbEM",
        "outputId": "fb4bd322-5969-49bf-a823-2a0da36831fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words  = preprocessing(Query) #query is preprocessed."
      ],
      "metadata": {
        "id": "dOj8tro3IG0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overal_addition(x):\n",
        "  return sum(x) #returning the sum of iterators for each key after groupByKey."
      ],
      "metadata": {
        "id": "ZyEajNjIAirj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = RDD.filter(lambda x : x[0] in words) #filtering our rdd only when words are in the set."
      ],
      "metadata": {
        "id": "8e1W9MuzBXvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RDD.take(1)"
      ],
      "metadata": {
        "id": "grObk0wSBYtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = RDD.flatMap(lambda x : [(document, x[1][document]) for document in x[1]]) #As we have every element of the form tuple of the form (word,{doc-id1:freq1}...) But our motive is to get documents so iterating in lambda function and taking that key,value pair as tuple and flattening the rdd.\n",
        "#RDD = RDD.map(lambda x: (x[0], x[1]))\n",
        "#RDD = RDD.reduceByKey(lambda a, b: a + b)\n",
        "RDD = RDD.groupByKey() # for same doc-id as key we group all freq values .\n",
        "RDD = RDD.mapValues(lambda x : overal_addition(x)) #As for common key because of groupbyKey we get a iterator and we will add by passing to mapvalues."
      ],
      "metadata": {
        "id": "WsDOYoSRKxbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RDD.take(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c2zWhEIBDA6",
        "outputId": "47bda8ac-a72f-4fbd-91ce-d7af950d6222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('doc-10000', 1.6989700043360187),\n",
              " ('doc-10049', 1.0),\n",
              " ('doc-10244', 1.3010299956639813),\n",
              " ('doc-10557', 1.6020599913279623)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OutputRDD = RDD.sortBy(lambda x: x[1], ascending=False) #from pyspark documentation #using lambda function based on values sorted in descending order."
      ],
      "metadata": {
        "id": "DA_VxUfcO0NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OutputRDD = OutputRDD.zipWithIndex()\n",
        "#To get first 10 elements of RDD without getting out of rdd used zipindex which gives indexing for every elements."
      ],
      "metadata": {
        "id": "05VsoD8dUMkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OutputRDD = OutputRDD.filter(lambda x : x[1]<=9) # taking first 10 document ids based on ids."
      ],
      "metadata": {
        "id": "n1Hg7TS8VJ46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OutputRDD.count()"
      ],
      "metadata": {
        "id": "84FYozA8VLFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OutputRDD = OutputRDD.map(lambda x : (x[0][0],x[0][1])) #taking as tuples"
      ],
      "metadata": {
        "id": "af1v-_AnVTiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OutputRDD.take(10)\n",
        "import os\n",
        "folder_path = \"/content/Task1Output1\"\n",
        "OutputRDD.saveAsTextFile(folder_path) #writing back to specifed folder."
      ],
      "metadata": {
        "id": "Evx5sDKdVjEP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}