{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F_APgb3siF7",
        "outputId": "8b686e70-5dca-4fa3-aabe-2eb98ba36841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark #pyspark is installed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf #packages are imported."
      ],
      "metadata": {
        "id": "SeUVWslLudPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf()\n",
        "conf.setMaster('local')\n",
        "conf.setAppName('CTFIDF_query')\n",
        "sc = SparkContext(conf=conf) #spark is necessiated."
      ],
      "metadata": {
        "id": "HOOtFdc8uiUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.rdd import RDD\n",
        "file2=\"/content/CTFIDF_index\" \n",
        "import os\n",
        "filename = os.path.basename(file2) #will take file name from the path.\n",
        "\n",
        "if str(filename) != \"CTFIDF_index\":\n",
        "  print(\"For this task you need to give only CTFIDF_index file , please give that file only\") # if filename is not CTFIDF_index program will be executed.\n",
        "  exit()\n",
        "else:\n",
        "  fileRDD = sc.textFile(file2) #parallelizing file to rdd.\n",
        "  def conversion_to_rdd(x): #this process is used to convert back to rdd.\n",
        "    y = x.split('@') #as we have every line wrod@doc-id1#freq1..... splitting on @ to get word in first element of list and remaining in second part.\n",
        "    z = y[1].split('+') # Again splitting on + as to represent 2 different documents.\n",
        "    document_dictionary ={} #initialinzing dictionary for storing doc-id as key and freq as value.\n",
        "    for t in z :\n",
        "      n = t.split('#')\n",
        "      document_dictionary[n[0]] = float(n[1]) #making freq to floats for further processing \n",
        "\n",
        "    return (y[0],document_dictionary) #returning a tuple in which first element is word and second element is dictionary of docids as keys and freq as values.\n",
        "\n",
        "  RDD = fileRDD.map(lambda x : conversion_to_rdd(x))\n",
        "  #RDD.take(1)\n",
        "  ID =0\n",
        "  while True: #infinite loop untill user says quits is given.\n",
        "    \n",
        "    Query = input('Enter your query for CTFIDF :- ')\n",
        "\n",
        "    from tkinter.constants import X\n",
        "    import re\n",
        "    import string #necessary packages are imported.\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    from nltk.corpus import stopwords\n",
        "    stop_words = list(stopwords.words('english')) #stopwords list is created for removal .\n",
        "    def preprocessing(x):\n",
        "      x = x.lower()  #query words are lowered \n",
        "      array1 = x.split(\" \") #splitted based on empty space.\n",
        "      y = \"\"\n",
        "      for x1 in array1: \n",
        "        if x1 in stop_words: #stopwords are removed if they are in the list of stopwords.\n",
        "          continue\n",
        "        else:\n",
        "          y = y+ \" \"+ x1 + \" \"\n",
        "\n",
        "      urls = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "      text_without_urls = urls.sub('', y) #query urls are removed\n",
        "\n",
        "      text_without_punctuations = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text_without_urls) #https://www.geeksforgeeks.org/python-remove-punctuation-from-string/ #query punctuations are removed.\n",
        "\n",
        "      useful_words = set()\n",
        "      array2 = text_without_punctuations.split(\" \")\n",
        "      for x1 in array2:\n",
        "        if len(x1)>0:  #I used set to add filtered words.\n",
        "          useful_words.add(x1)\n",
        "\n",
        "      return useful_words #that set is returned.\n",
        "\n",
        "\n",
        "    words  = preprocessing(Query) #query is preprocessed.\n",
        "\n",
        "    CTFIDFRDD = RDD.filter(lambda x : x[0] in words) #filtering our rdd only when words are in the set.\n",
        "\n",
        "    import math\n",
        "    def construction_of_IDF_and_TFIDF(x):\n",
        "      no_of_documents_with_one_word = len(x[1]) #length of dictionary to calculate IDF which gives you no of documents the word is present.\n",
        "      top =10000/(1+ no_of_documents_with_one_word) # here idf is calculated for every word and 10000 is standard total documents which is given in the question.\n",
        "      IDF = math.log(top,10) \n",
        "\n",
        "      output = []\n",
        "      for y in x[1]:\n",
        "        TF_IDF_of_particular_document = x[1][y] * IDF\n",
        "        per_document = (y,IDF,TF_IDF_of_particular_document) #for every value TFIDF is calculated  and overridden in dictionary.\n",
        "        output.append(per_document)\n",
        "      return tuple(output) #returning as tuple of list of tuples in the form (doc-id,IDF,TFIDF OF every document)\n",
        "\n",
        "\n",
        "    CTFIDFRDD = CTFIDFRDD.map(lambda x : construction_of_IDF_and_TFIDF(x)) #rdd will be of the tuple of tuple form (((doc-id,IDF,TFIDF),(doc-id2,IDF2)...))\n",
        "    def conversion_to_dictionary(x):\n",
        "      output ={} #we get every line as (((doc-id,IDF,TFIDF),(doc-id2,IDF2)...))\n",
        "      for y in x :\n",
        "        values=(y[1],y[2])\n",
        "        output[y[0]] = values\n",
        "      return output #we wrill return dictionary in which elements will be of the form {doc-id:(IDF,TFIDF)}\n",
        "\n",
        "    CTFIDFRDD = CTFIDFRDD.map(lambda x : conversion_to_dictionary(x)) #rdd is mapped based on the return values of convert_to_dictionary function.\n",
        "    CTFIDFRDD = CTFIDFRDD.flatMap(lambda x: [(doc_id, x[doc_id]) for doc_id  in x]) #As we have every element of the form tuple of the form (word,{doc-id1:(IDF,TFIDF)}...) But our motive is to get documents so iterating in lambda function and taking that key,value pair as tuple and flattening the rdd.\n",
        "    CTFIDFRDD = CTFIDFRDD.groupByKey() ## for same doc-id as key we group all tuples of the form (IDF,TFIDF) \n",
        "    CTFIDFRDD = CTFIDFRDD.mapValues(list) # we make the iterator of every key to be list.\n",
        "    def Dot_product(x):\n",
        "      dot_product = 0\n",
        "      for y in x[1]:\n",
        "        ct   = y[0]\n",
        "        ctfd = y[1]\n",
        "        dot_product += ct*ctfd #performing dot product and adding all the vectors.\n",
        "      return (x[0],dot_product)\n",
        "\n",
        "    CTFIDFRDD = CTFIDFRDD.map(lambda x : Dot_product(x)) #we get rdd of elements of type (doc-id, dot producted value) after calling the function.\n",
        "\n",
        "    OutputRDD = CTFIDFRDD.sortBy(lambda x: x[1], ascending=False) #from pyspark documentation ##from pyspark documentation #using lambda function based on values sorted in descending order.\n",
        "    OutputRDD = OutputRDD.zipWithIndex() # To get first 10 elements of RDD without getting out of rdd used zipindex which gives indexing for every elements.\n",
        "    OutputRDD = OutputRDD.filter(lambda x : x[1]<=9) # taking first 10 document ids based on ids.\n",
        "    OutputRDD = OutputRDD.map(lambda x : (x[0][0],x[0][1])) #taking as tuples.\n",
        "\n",
        "    ID +=1 #incrementing ID because for every ID we need folder.\n",
        "    foldername = \"q\"+str(ID)\n",
        "    #os.mkdir(foldername)\n",
        "    \n",
        "    #OutputRDD.saveAsTextFile(folder_path)\n",
        "    \n",
        "\n",
        "    root_path = \"/content/\"  \n",
        "    OutputRDD.saveAsTextFile(root_path + \"/\" + foldername) #for the root path we will add which id query is that.\n",
        "\n",
        "    #global ID \n",
        "    \n",
        "\n",
        "\n",
        "    user_input = input(\"Type 'quit' to exit: \") #when user wishes to quit. he should type quit.\n",
        "    if user_input == \"quit\":\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0RWGmfHurFH",
        "outputId": "cc05ec65-424c-445f-e3c6-917665a03ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query for CTFIDF :- JERUSALEM in muslims hates NA3zi in www.google.com\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type 'quit' to exit: start\n",
            "Enter your query for CTFIDF :- war regarding beer is always hating\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type 'quit' to exit: start\n",
            "Enter your query for CTFIDF :- hello seige war is cold blooded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type 'quit' to exit: quit\n"
          ]
        }
      ]
    }
  ]
}